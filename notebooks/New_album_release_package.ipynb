{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "4025a4e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#data                               \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from wordcloud import WordCloud, ImageColorGenerator\n",
    "import base64\n",
    "from io import BytesIO\n",
    "from PIL import Image\n",
    "import requests\n",
    "\n",
    "#nlp\n",
    "import re\n",
    "import string\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# wordcloud\n",
    "from PIL import Image\n",
    "from wordcloud import STOPWORDS, WordCloud\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from wordcloud import WordCloud, ImageColorGenerator\n",
    "import base64\n",
    "from io import BytesIO\n",
    "from PIL import Image\n",
    "import requests\n",
    "\n",
    "#data                               \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "#nlp\n",
    "import re\n",
    "import string\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "# from nltk.stem import PorterStemmer\n",
    "# nltk.download('wordnet')\n",
    "# nltk.download('omw-1.4')\n",
    "# nltk.download('punkt')\n",
    "# nltk.download('stopwords')\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "# from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "# wordcloud\n",
    "from PIL import Image\n",
    "from wordcloud import STOPWORDS, WordCloud\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "104915bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fcddffa",
   "metadata": {},
   "source": [
    "# Load Original dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "a986a992",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>artist_name</th>\n",
       "      <th>album_id</th>\n",
       "      <th>album_title</th>\n",
       "      <th>release_date</th>\n",
       "      <th>year</th>\n",
       "      <th>song_lyrics</th>\n",
       "      <th>album_image</th>\n",
       "      <th>eng</th>\n",
       "      <th>clean_eng</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>artist_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>20056456</th>\n",
       "      <td>(G)I-DLE</td>\n",
       "      <td>20345924</td>\n",
       "      <td>the baddest</td>\n",
       "      <td>2020-08-29</td>\n",
       "      <td>2020</td>\n",
       "      <td>baddest baddest baddest baddest ooh baddest ba...</td>\n",
       "      <td>https://image.bugsm.co.kr/album/images/200/203...</td>\n",
       "      <td>baddest baddest baddest baddest ooh baddest ba...</td>\n",
       "      <td>baddest baddest baddest baddest baddest baddes...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20056456</th>\n",
       "      <td>(G)I-DLE</td>\n",
       "      <td>20316781</td>\n",
       "      <td>i trust</td>\n",
       "      <td>2020-04-06</td>\n",
       "      <td>2020</td>\n",
       "      <td>nothing pull away embrace going anywhere go he...</td>\n",
       "      <td>https://image.bugsm.co.kr/album/images/200/203...</td>\n",
       "      <td>nothing pull away embrace going anywhere go he...</td>\n",
       "      <td>nothing pull away embrace go anywhere go hear ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20056456</th>\n",
       "      <td>(G)I-DLE</td>\n",
       "      <td>20262506</td>\n",
       "      <td>uh-oh</td>\n",
       "      <td>2019-06-26</td>\n",
       "      <td>2019</td>\n",
       "      <td>거기 누군가요 안다고요 잊어버렸죠 버렸죠 너도 웃기지 않나요 당황스럽네요 친한 척 ...</td>\n",
       "      <td>https://image.bugsm.co.kr/album/images/200/202...</td>\n",
       "      <td>Is there someone there? I know. I forgot. I th...</td>\n",
       "      <td>someone know forget throw away arent funny emb...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20056456</th>\n",
       "      <td>(G)I-DLE</td>\n",
       "      <td>20232946</td>\n",
       "      <td>i made</td>\n",
       "      <td>2019-02-26</td>\n",
       "      <td>2019</td>\n",
       "      <td>unsteady uncertain 그저 원해 stay calm 그림과 같아 ever...</td>\n",
       "      <td>https://image.bugsm.co.kr/album/images/200/202...</td>\n",
       "      <td>unsteady uncertain I just want stay calm Its l...</td>\n",
       "      <td>unsteady uncertain want stay calm like picture...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20056456</th>\n",
       "      <td>(G)I-DLE</td>\n",
       "      <td>20206916</td>\n",
       "      <td>달려! (relay)</td>\n",
       "      <td>2018-11-19</td>\n",
       "      <td>2018</td>\n",
       "      <td>심장이 쿵쿵 신호탄 뱅뱅 이상 뒤로 물러설 공간 따윈 없어 발소리 쿵쿵 환호성 예예...</td>\n",
       "      <td>https://image.bugsm.co.kr/album/images/200/202...</td>\n",
       "      <td>My heart is pounding, the signal is bang bang,...</td>\n",
       "      <td>heart pound signal bang bang room step back so...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80073013</th>\n",
       "      <td>miss A</td>\n",
       "      <td>321783</td>\n",
       "      <td>touch</td>\n",
       "      <td>2012-02-20</td>\n",
       "      <td>2012</td>\n",
       "      <td>닫힌 가슴은 누구도 사랑할 수가 없다 그렇게 믿었는데 어느새 가슴이 열리고 있어 굳...</td>\n",
       "      <td>https://image.bugsm.co.kr/album/images/200/321...</td>\n",
       "      <td>I believed that no one can love a closed heart...</td>\n",
       "      <td>believe one love close heart heart open hard h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80073013</th>\n",
       "      <td>miss A</td>\n",
       "      <td>344367</td>\n",
       "      <td>independent women pt.ⅲ</td>\n",
       "      <td>2012-10-15</td>\n",
       "      <td>2012</td>\n",
       "      <td>아쉬울 것 없어 이럴 시간 없어 이젠 괜찮아 귀찮았어 귀찮았어 솔직해져도 돼 이럴 ...</td>\n",
       "      <td>https://image.bugsm.co.kr/album/images/200/344...</td>\n",
       "      <td>Theres nothing to regret, theres no time for t...</td>\n",
       "      <td>there nothing regret there time okay annoy ann...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80073013</th>\n",
       "      <td>miss A</td>\n",
       "      <td>395499</td>\n",
       "      <td>hush</td>\n",
       "      <td>2013-11-06</td>\n",
       "      <td>2013</td>\n",
       "      <td>숨소리가 들려 몸이 녹아내려 견딜 수가 없어 참을 수가 없어 숨이 달아 올라 자꾸 ...</td>\n",
       "      <td>https://image.bugsm.co.kr/album/images/200/395...</td>\n",
       "      <td>I hear the sound of my breathing, my body is m...</td>\n",
       "      <td>hear sound breathe body melt cant bear cant st...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20158908</th>\n",
       "      <td>LE SSERAFIM (르세라핌)</td>\n",
       "      <td>4092452</td>\n",
       "      <td>Perfect Night</td>\n",
       "      <td>2023.10.27</td>\n",
       "      <td>2023</td>\n",
       "      <td>ya ya drama ziggy ziggy zag new cause go biggi...</td>\n",
       "      <td>https://image.bugsm.co.kr/album/images/200/409...</td>\n",
       "      <td>ya ya drama ziggy ziggy zag new cause go biggi...</td>\n",
       "      <td>drama ziggy ziggy zag new go biggie biggie bad...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20158908</th>\n",
       "      <td>LE SSERAFIM (르세라핌)</td>\n",
       "      <td>4092452</td>\n",
       "      <td>Perfect Night</td>\n",
       "      <td>2023-10-27 00:00:00</td>\n",
       "      <td>2023</td>\n",
       "      <td>ya ya drama ziggy ziggy zag new cause go biggi...</td>\n",
       "      <td>https://image.bugsm.co.kr/album/images/200/409...</td>\n",
       "      <td>ya ya drama ziggy ziggy zag new cause go biggi...</td>\n",
       "      <td>drama ziggy ziggy zag new go biggie biggie bad...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>590 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  artist_name  album_id             album_title  \\\n",
       "artist_id                                                         \n",
       "20056456             (G)I-DLE  20345924             the baddest   \n",
       "20056456             (G)I-DLE  20316781                 i trust   \n",
       "20056456             (G)I-DLE  20262506                   uh-oh   \n",
       "20056456             (G)I-DLE  20232946                  i made   \n",
       "20056456             (G)I-DLE  20206916             달려! (relay)   \n",
       "...                       ...       ...                     ...   \n",
       "80073013               miss A    321783                   touch   \n",
       "80073013               miss A    344367  independent women pt.ⅲ   \n",
       "80073013               miss A    395499                    hush   \n",
       "20158908   LE SSERAFIM (르세라핌)   4092452           Perfect Night   \n",
       "20158908   LE SSERAFIM (르세라핌)   4092452           Perfect Night   \n",
       "\n",
       "                  release_date  year  \\\n",
       "artist_id                              \n",
       "20056456            2020-08-29  2020   \n",
       "20056456            2020-04-06  2020   \n",
       "20056456            2019-06-26  2019   \n",
       "20056456            2019-02-26  2019   \n",
       "20056456            2018-11-19  2018   \n",
       "...                        ...   ...   \n",
       "80073013            2012-02-20  2012   \n",
       "80073013            2012-10-15  2012   \n",
       "80073013            2013-11-06  2013   \n",
       "20158908            2023.10.27  2023   \n",
       "20158908   2023-10-27 00:00:00  2023   \n",
       "\n",
       "                                                 song_lyrics  \\\n",
       "artist_id                                                      \n",
       "20056456   baddest baddest baddest baddest ooh baddest ba...   \n",
       "20056456   nothing pull away embrace going anywhere go he...   \n",
       "20056456   거기 누군가요 안다고요 잊어버렸죠 버렸죠 너도 웃기지 않나요 당황스럽네요 친한 척 ...   \n",
       "20056456   unsteady uncertain 그저 원해 stay calm 그림과 같아 ever...   \n",
       "20056456   심장이 쿵쿵 신호탄 뱅뱅 이상 뒤로 물러설 공간 따윈 없어 발소리 쿵쿵 환호성 예예...   \n",
       "...                                                      ...   \n",
       "80073013   닫힌 가슴은 누구도 사랑할 수가 없다 그렇게 믿었는데 어느새 가슴이 열리고 있어 굳...   \n",
       "80073013   아쉬울 것 없어 이럴 시간 없어 이젠 괜찮아 귀찮았어 귀찮았어 솔직해져도 돼 이럴 ...   \n",
       "80073013   숨소리가 들려 몸이 녹아내려 견딜 수가 없어 참을 수가 없어 숨이 달아 올라 자꾸 ...   \n",
       "20158908   ya ya drama ziggy ziggy zag new cause go biggi...   \n",
       "20158908   ya ya drama ziggy ziggy zag new cause go biggi...   \n",
       "\n",
       "                                                 album_image  \\\n",
       "artist_id                                                      \n",
       "20056456   https://image.bugsm.co.kr/album/images/200/203...   \n",
       "20056456   https://image.bugsm.co.kr/album/images/200/203...   \n",
       "20056456   https://image.bugsm.co.kr/album/images/200/202...   \n",
       "20056456   https://image.bugsm.co.kr/album/images/200/202...   \n",
       "20056456   https://image.bugsm.co.kr/album/images/200/202...   \n",
       "...                                                      ...   \n",
       "80073013   https://image.bugsm.co.kr/album/images/200/321...   \n",
       "80073013   https://image.bugsm.co.kr/album/images/200/344...   \n",
       "80073013   https://image.bugsm.co.kr/album/images/200/395...   \n",
       "20158908   https://image.bugsm.co.kr/album/images/200/409...   \n",
       "20158908   https://image.bugsm.co.kr/album/images/200/409...   \n",
       "\n",
       "                                                         eng  \\\n",
       "artist_id                                                      \n",
       "20056456   baddest baddest baddest baddest ooh baddest ba...   \n",
       "20056456   nothing pull away embrace going anywhere go he...   \n",
       "20056456   Is there someone there? I know. I forgot. I th...   \n",
       "20056456   unsteady uncertain I just want stay calm Its l...   \n",
       "20056456   My heart is pounding, the signal is bang bang,...   \n",
       "...                                                      ...   \n",
       "80073013   I believed that no one can love a closed heart...   \n",
       "80073013   Theres nothing to regret, theres no time for t...   \n",
       "80073013   I hear the sound of my breathing, my body is m...   \n",
       "20158908   ya ya drama ziggy ziggy zag new cause go biggi...   \n",
       "20158908   ya ya drama ziggy ziggy zag new cause go biggi...   \n",
       "\n",
       "                                                   clean_eng  \n",
       "artist_id                                                     \n",
       "20056456   baddest baddest baddest baddest baddest baddes...  \n",
       "20056456   nothing pull away embrace go anywhere go hear ...  \n",
       "20056456   someone know forget throw away arent funny emb...  \n",
       "20056456   unsteady uncertain want stay calm like picture...  \n",
       "20056456   heart pound signal bang bang room step back so...  \n",
       "...                                                      ...  \n",
       "80073013   believe one love close heart heart open hard h...  \n",
       "80073013   there nothing regret there time okay annoy ann...  \n",
       "80073013   hear sound breathe body melt cant bear cant st...  \n",
       "20158908   drama ziggy ziggy zag new go biggie biggie bad...  \n",
       "20158908   drama ziggy ziggy zag new go biggie biggie bad...  \n",
       "\n",
       "[590 rows x 9 columns]"
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('../data/updated_df_111123.csv', index_col = 0)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "aec92623",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ya29.c.c0AY_VpZhQ22upRvOSBeUy8fke3K65Uf8i-yhSo9_aqv_gR4CVJc3kfyB4JVJXx8gN9Kk6oIWlQmmmkBQL7Qmv-6Yh9SPJvYaBQUcTwsNYRdODAlRQbp0Gosw2wX-Kg2OuCrYpBesFzkRE04DicD-pSZgfrvTtP-Lx-Gz95-hVspVUIEUZ4iRttRgfkyIEfOMYGyp30OmX0I3PW751pU58NfdQLOte7Tj5BEKEpHq--OUSDgvpmE-732MqJzs-GKBfRD3m0RbXJyGBszdSg7r0m3u5y9GcyKzgtHKGO7pR9dH3retHwKmd3RbBFh_klVDO0N7Z4aRXVsQljY83b6Y31aBAY5CCzX4EB8skjsdkKmq-ZLIhyGaIWMI4E385AYVYv2Wj6iyvI9985Bazgh4Q3X060cZ-5s9mBObjZ-oFRXSXrF5MJWZzezmxq0e8bezbF63sn9rB-qsQSnRM3quyg1j4oQfmUcX3IV43n6eOYuIS5yJ6Y6nsqMBy5R6sOwySsvcoR7uuux9vSQ1URMJJRR2Uvoo91Xv149IWB77cdxbY_0yuWk_2WpntfOUUiZgS8zBzujwwS0Wu0r23R306mocZbktwRZk3moWtpJ710pz2zeulUcU_whglq7gm7Uxssk5Jw9b1kmIFiic_YUJsvbVX7va3IdkjZupvj6r0RaZV4Qz8pS65_eB0YS90Qipbq2fRi389JMtjUS5iImeoVoSQ9u1kx698XmeSzv3aql8oq4gJn0Vpzn26d0BQ6rihV0Jd37Bo2-hjpqF3UzelI-JUiwtlngz4lRe556SFcFI0iYmehZ78OjZmfVhrl9lzVVRrsm1XtsXj9hq-6wcJ8B0rruxVrlIIur-9hV5JcRuZJu43foZjMRpFz62Jjbhgtb9WItFB5y-oZMw_2UjyWSkr2pdaun-iQwBzSYJnjp22vastha3hBW-br5nhui9vj-Uu_v_4l2Ommj-2obmet9SiSjt4jngc9QZkvBSYtgw-h8FO6VpkBas\n",
      "\n",
      "\n",
      "Updates are available for some Google Cloud CLI components.  To install them,\n",
      "please run:\n",
      "  $ gcloud components update\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!gcloud auth application-default print-access-token"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36b1d6aa",
   "metadata": {},
   "source": [
    "# Scrape new data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99e00f29",
   "metadata": {},
   "source": [
    "## Lyrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "8ec921b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def new_album_lyrics_scraping(album_id_):\n",
    "    url = f'https://music.bugs.co.kr/album/{album_id_}'\n",
    "    request = requests.get(url)\n",
    "    html = request.text\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "    artist_id = [] #\n",
    "    artist_name = [] #\n",
    "    album_id = [album_id_] #\n",
    "    album_title = [] #\n",
    "    release_date=[] #\n",
    "    year = [] #\n",
    "    song_lyrics = [] \n",
    "    album_image = [] #\n",
    "    eng = []\n",
    "    clean_eng = []\n",
    "\n",
    "    song_id = [] #\n",
    "    song_title = [] #\n",
    "\n",
    "    basic_info_table = soup.find(\"div\", attrs={'class':'basicInfo'})\n",
    "\n",
    "    for elem in basic_info_table.find_all('a'):\n",
    "         if \"https://music.bugs.co.kr/artist/\" in elem['href']:\n",
    "            artist_id.append(elem['href'].strip('https://music.bugs.co.kr/artist/').split('?')[0])\n",
    "            artist_name.append(elem.text.strip())\n",
    "\n",
    "\n",
    "    for elem in soup.find_all('header', attrs = {'class': 'sectionPadding pgTitle noneLNB'}):\n",
    "        album_title.append(elem.find('h1').text)\n",
    "\n",
    "    album_image.append(basic_info_table.find(\"div\", attrs={'class':'photos'}).find(\"img\")['src'])\n",
    "    release_date.append(basic_info_table.find_all('time')[0].text)\n",
    "\n",
    "    year.append(basic_info_table.find_all('time')[0].text.split('.')[0])\n",
    "\n",
    "    album_info_table = soup.find(\"table\", attrs={'class':'list trackList byAlbum'})\n",
    "    \n",
    "    #songs\n",
    "\n",
    "    for elem in album_info_table.find_all('tr'):\n",
    "        if elem.find('a') != None:\n",
    "            song_id.append(elem.find('a')['href'].strip('https://music.bugs.co.kr/track/').split('?')[0])\n",
    "\n",
    "    for elem in album_info_table.find_all('p', attrs = {'class': 'title'}):\n",
    "        song_title.append(elem.find('a').text)\n",
    "    \n",
    "    \n",
    "    temp_lyrics = []\n",
    "    for s in song_id:\n",
    "        l_url = f'https://music.bugs.co.kr/track/{s}'\n",
    "        l_request = requests.get(l_url)\n",
    "        l_html = l_request.text\n",
    "        l_soup = BeautifulSoup(l_html, \"html.parser\")\n",
    "        lyric = str(l_soup.select(\n",
    "                '#container > section.sectionPadding.contents.lyrics > div.innerContainer > div.lyricsContainer > p > xmp'))\n",
    "        lyric = re.sub('<.+?>', '', lyric, 0)\n",
    "        temp_lyrics.append(lyric)\n",
    "        \n",
    "    song_lyrics.append(' '.join(temp_lyrics))\n",
    "    \n",
    "    \n",
    "    def preprocessing(sentence):\n",
    "        sentence = str(sentence)\n",
    "        sentence = sentence.strip()\n",
    "        sentence = sentence.lower()\n",
    "        sentence = ''.join(char for char in sentence if not char.isdigit())\n",
    "        sentence = sentence.replace('\\r\\n', ' ')\n",
    "\n",
    "\n",
    "        manual_sw = ['amp', '-', 'na', 'ah', 'uh', 'eh', 'la', 'ta', 'oh', 'du', 'yeah', '’', \n",
    "                    'a','b','c','d','e','f','g','h','i','j','k','l','m','n','o','p',\n",
    "                     'q','r','s','t','u','v','w','x','y','z']\n",
    "\n",
    "        for punctuation in string.punctuation:\n",
    "            sentence = sentence.replace(punctuation, ' ') \n",
    "\n",
    "        tokenized_sentence = word_tokenize(sentence) ## tokenize \n",
    "        stop_words = list(set(stopwords.words('english'))) ## define stopwords\n",
    "        stop_words.extend(manual_sw)\n",
    "\n",
    "        tokenized_sentence_cleaned = [ ## remove stopwords\n",
    "            w for w in tokenized_sentence if not w in stop_words\n",
    "        ]\n",
    "\n",
    "        return ' '.join(tokenized_sentence_cleaned)\n",
    "\n",
    "    \n",
    "    def translate_text(text: str) -> dict:\n",
    "        \"\"\"Translates text into the target language.\n",
    "\n",
    "        Target must be an ISO 639-1 language code.\n",
    "        See https://g.co/cloud/translate/v2/translate-reference#supported_languages\n",
    "        \"\"\"\n",
    "        from google.cloud import translate_v2 as translate\n",
    "\n",
    "        translate_client = translate.Client()\n",
    "\n",
    "        if isinstance(text, bytes):\n",
    "            text = text.decode(\"utf-8\")\n",
    "\n",
    "        # Text can also be a sequence of strings, in which case this method\n",
    "        # will return a sequence of results for each text.\n",
    "        result = translate_client.translate(text, target_language='en')\n",
    "\n",
    "    #     print(\"Text: {}\".format(result[\"input\"]))\n",
    "    #     print(\"Translation: {}\".format(result[\"translatedText\"]))\n",
    "    #     print(\"Detected source language: {}\".format(result[\"detectedSourceLanguage\"]))\n",
    "\n",
    "        return result[\"translatedText\"]\n",
    "    \n",
    "    \n",
    "    def preprocessing_eng(sentence):\n",
    "        sentence = str(sentence)\n",
    "        sentence = sentence.strip()\n",
    "        sentence = sentence.lower()\n",
    "        sentence = ''.join(char for char in sentence if not char.isdigit())\n",
    "        sentence = sentence.replace('\\r\\n', ' ')\n",
    "\n",
    "\n",
    "        manual_sw = ['amp','na', 'ah', 'uh', 'eh', 'la', 'ta', 'oh', 'du', 'yeah', '’', '-',\n",
    "                    '“', 'wan', 'woo', 'ay', 'whoa', 'im', 'gon', 'nah',\n",
    "                     'im', 'ye', 'ha', 'ho', 'ill', 'doo', 'ya', 'ga',\n",
    "                     'let', 'hey',  'verse', 'chorus', 'dum', \n",
    "                     'thing', 'try', 'yah', 'cause', 'da', 'ba', 'ooh', 'ti', \n",
    "                     'tu', 'ch', 'yuh', 'um', 'ey'\n",
    "                    'a','b','c','d','e','f','g','h','i','j','k','l','m','n','o','p',\n",
    "                     'q','r','s','t','u','v','w','x','y','z']\n",
    "                    #'dont', 'cant','take', 'got', 'say']\n",
    "\n",
    "        for punctuation in string.punctuation:\n",
    "            sentence = sentence.replace(punctuation, ' ') \n",
    "\n",
    "        tokenized_sentence = word_tokenize(sentence) ## tokenize \n",
    "        stop_words = list(set(stopwords.words('english'))) ## define stopwords\n",
    "        stop_words.extend(manual_sw)\n",
    "\n",
    "        tokenized_sentence_cleaned = [w for w in tokenized_sentence if not w in stop_words]\n",
    "\n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "        lemmatized_sentence_n = [lemmatizer.lemmatize(word, pos='n') for word in tokenized_sentence_cleaned]\n",
    "        lemmatized_sentence_v = [lemmatizer.lemmatize(word, pos='v') for word in lemmatized_sentence_n]\n",
    "\n",
    "\n",
    "        return ' '.join(lemmatized_sentence_v)\n",
    "\n",
    "\n",
    "    column_list = {\n",
    "        'artist_id': artist_id, \n",
    "        'artist_name': artist_name, \n",
    "        'album_id':album_id, \n",
    "        'album_title':album_title, \n",
    "        'release_date': release_date,\n",
    "        'year': year, \n",
    "        'song_lyrics': song_lyrics, \n",
    "        'album_image': album_image, \n",
    "        'eng': eng, \n",
    "        'clean_eng': clean_eng\n",
    "    }\n",
    "\n",
    "    df = pd.DataFrame.from_dict(column_list, orient='index') # dict를 dataframe 형식으로 변환\n",
    "    df = df.transpose()\n",
    "    \n",
    "    \n",
    "    df['song_lyrics'] = df['song_lyrics'].apply(preprocessing)\n",
    "    df['eng'] = df['song_lyrics'].apply(translate_text)\n",
    "    df['eng'] = df['eng'].str.replace(\"&#39;\",\"\")\n",
    "\n",
    "    df['clean_eng'] = df['eng'].apply(preprocessing_eng)\n",
    "    \n",
    "    df['release_date'] = pd.to_datetime(df['release_date'])\n",
    "    \n",
    "    \n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "id": "09c89695",
   "metadata": {},
   "outputs": [],
   "source": [
    "album_id_= 4092948"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "id": "28c7402d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>artist_id</th>\n",
       "      <th>artist_name</th>\n",
       "      <th>album_id</th>\n",
       "      <th>album_title</th>\n",
       "      <th>release_date</th>\n",
       "      <th>year</th>\n",
       "      <th>song_lyrics</th>\n",
       "      <th>album_image</th>\n",
       "      <th>eng</th>\n",
       "      <th>clean_eng</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>564</th>\n",
       "      <td>80347326</td>\n",
       "      <td>aespa</td>\n",
       "      <td>4092948</td>\n",
       "      <td>Drama - The 4th Mini Album</td>\n",
       "      <td>2023.11.10</td>\n",
       "      <td>2023</td>\n",
       "      <td>ya ya drama ziggy ziggy zag new cause go biggi...</td>\n",
       "      <td>https://image.bugsm.co.kr/album/images/200/409...</td>\n",
       "      <td>ya ya drama ziggy ziggy zag new cause go biggi...</td>\n",
       "      <td>drama ziggy ziggy zag new go biggie biggie bad...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    artist_id artist_name album_id                 album_title release_date  \\\n",
       "564  80347326       aespa  4092948  Drama - The 4th Mini Album   2023.11.10   \n",
       "\n",
       "     year                                        song_lyrics  \\\n",
       "564  2023  ya ya drama ziggy ziggy zag new cause go biggi...   \n",
       "\n",
       "                                           album_image  \\\n",
       "564  https://image.bugsm.co.kr/album/images/200/409...   \n",
       "\n",
       "                                                   eng  \\\n",
       "564  ya ya drama ziggy ziggy zag new cause go biggi...   \n",
       "\n",
       "                                             clean_eng  \n",
       "564  drama ziggy ziggy zag new go biggie biggie bad...  "
      ]
     },
     "execution_count": 234,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfdfdf = new_df[new_df['album_id']==album_id_]\n",
    "artist_id_ = dfdfdf.iloc[0]['artist_id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "id": "702a7b2e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "80347326"
      ]
     },
     "execution_count": 238,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "artist_id_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "id": "1544cf3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def wordcloud_for_new_data(df, album_id_):\n",
    "    new_df = df[df['album_id']==album_id_]\n",
    "    \n",
    "    artist_id_ = new_df.iloc[0]['artist_id']\n",
    "\n",
    "    for idx, (album_image, lyrics) in enumerate(zip(new_df['album_image'], new_df['clean_eng']), start=1):        \n",
    "\n",
    "        response = requests.get(album_image)\n",
    "        image_data = BytesIO(response.content)\n",
    "        image_opened = Image.open(image_data)\n",
    "        image_colors = ImageColorGenerator(np.array(image_opened), default_color = (255,255,255))\n",
    "        mask = np.array(Image.open(os.path.join(os.getcwd(), '../frontend/data/masking_image.jpeg')))\n",
    "\n",
    "        word_cloud = WordCloud(\n",
    "            width=200,\n",
    "            height=200,\n",
    "            max_words=40,\n",
    "            mask=mask,\n",
    "            collocations=False,\n",
    "            background_color=\"rgba(255, 255, 255, 0)\",\n",
    "            mode=\"RGBA\",\n",
    "            font_path='../data/font/JalnanOTF.otf'\n",
    "        ).generate(lyrics)\n",
    "\n",
    "        image = word_cloud.recolor(color_func=image_colors).to_image()\n",
    "\n",
    "        image.save(f'../frontend/wordcloud/eng_lemma/{artist_id_}_{album_id_}_wc.png')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7ead738",
   "metadata": {},
   "source": [
    "aespa_drama_111123"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "3a2cf718",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_album_info = new_album_lyrics_scraping(4092948)\n",
    "new_df = pd.concat([df, new_album_info]).reset_index(drop=True)\n",
    "new_df.to_csv('../data/updated_df_111123.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "id": "7448aadd",
   "metadata": {},
   "outputs": [],
   "source": [
    "wordcloud_for_new_data(new_df, 4092948)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfea347b",
   "metadata": {},
   "source": [
    "leserrafim_perfectnight_111123"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "cc774a14",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../data/updated_df_111123.csv')\n",
    "new_album_info = new_album_lyrics_scraping(4092452)\n",
    "new_df = pd.concat([df, new_album_info]).reset_index(drop=True)\n",
    "new_df.to_csv('../data/updated_df_111123.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "id": "7720cfae",
   "metadata": {},
   "outputs": [],
   "source": [
    "wordcloud_for_new_data(new_df, 4092452)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f50a01d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
